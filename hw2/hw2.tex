%        File: hw2.tex
%     Created: Wed Oct 05 04:00 PM 2016 C
% Last Change: Wed Oct 05 04:00 PM 2016 C
%

\documentclass[a4paper]{article}

\title{Math 8651 Homework 2 }
\date{10/17/16}
\author{Trevor Steil}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{esint}
\usepackage{bbm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem*{claim}{Claim}
\newtheorem*{problem}{Problem}
%\newtheorem*{lemma}{Lemma}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\supp}[1]{\mathop{\mathrm{supp}}\left(#1\right)}
\newcommand{\lip}[1]{\mathop{\mathrm{Lip}}\left(#1\right)}
\newcommand{\curl}{\mathrm{curl}}
\newcommand{\la}{\left \langle}
\newcommand{\ra}{\right \rangle}
\renewcommand{\vec}[1]{\mathbf{#1}}

\newenvironment{solution}{\emph{Solution.}}

\begin{document}
\maketitle

\begin{enumerate}
  \item
    \begin{problem}

      Let $\{A_n\}$ be a countable collection of measurable sets with respect to the probability space $(\Omega, \mathcal{F}, P)$, and assume that
      there exists $\delta > 0$ such that $P(A_n) > \delta$ for all $n$. True or false: there is at least one point in $\Omega$ which belongs to
      infinitely many $A_n$.

    \end{problem}

    \begin{solution}
      %Define the random variables
      %\[ X_n = \mathbbm{1}_{A_n} \]
      %for all $n$. Let
      %\[ X = \sum_{i=1}^\infty X_i .\]
      %The partial sums $\sum_{i=1}^N X_i$ are an increasing sequence of random variables. Therefore,
      %\begin{align*}
      %  \E[X] &= \lim_{N \to \infty} \sum_{i=1}^N \E[X_i] \\
      %  &> \lim_{N \to \infty} \sum_{i=1}^N \delta \\
      %  &= \lim_{N \to \infty} N \delta \\
      %  &\to \infty
      %\end{align*}

      %Therefore, there is a set $B \subset \Omega$ such that $P(B) > 0$ and $X(\omega) = \infty$ for all $\omega \in B$.

      We are interested in the event that $\omega \in A_n$ for infinitely many $n$. We can look at
      \[ A = \limsup_{n \to \infty} A_n = \bigcap_{n=1}^\infty \bigcup_{k=n}^\infty A_n .\]

      Define the sets
      \[ B_n = \bigcup_{k=n}^\infty A_k \]
      for $n \geq 1$. Because we are taking unions of fewer sets, we see ${B_1 \supset B_2 \supset \dots}$.

      Therefore, by continuity of measure,
      \begin{align*}
        P(A) &= P \left( \bigcap_{n=1}^\infty \bigcup_{k=n}^\infty A_n \right) \\
        &= P \left( \bigcap_{n=1}^\infty B_n \right) \\
        &= \lim_{n \to \infty} P(B_n) \\
        &= \lim_{n \to \infty} P \left( \bigcup_{k=n}^\infty A_k \right) \\
        &\geq \delta
      \end{align*}
      because $P(A_k) > \delta$ for all $k$. Therefore, the set of $\omega$ such that $\omega \in A_n$ for infinitely many $n$ has positive measure,
      and there is at least one point in $\Omega$ which belongs to infinitely many $A_n$.

    \end{solution}

  \item
    \begin{problem}

      Assume $X \geq 0$, where $X$ is a real valued random variable. Is $\sigma^2(X \wedge y)$ an increasing function of $y$?

    \end{problem}

    \begin{solution}

      This statement is true. To show this, we will need to be able to compute moments.

      \begin{claim}
        Let $X$ be a random variable with $X \geq 0$. Then
        \[ \E[ X^n ] = n \int_{0}^{\infty} x^{n-1} (1 - P(\{ X \leq x \}) dx .\]
      \end{claim}
      \begin{proof}

        We can write
        \[ X^n = n \int_{0}^{X} x^{n-1} dx .\]
        Then
        \begin{align*}
          \E[X^n] &= \int_{\Omega}^{} X^n dP \\
          &= \int_{\Omega}^{} n \int_{0}^{X} x^{n-1} dx dP \\
          &= n \int_{0}^{\infty} \int_{\{X>x\}}^{} x^{n-1} dP dx \quad \parbox{5cm}{by Tonelli's Theorem}\\
          &= n \int_{0}^{\infty} x^{n-1} P(\{ X > x \}) dx \\
          &= n \int_{0}^{\infty} x^{n-1} (1 - P( \{ X \leq x \} ) dx
        \end{align*}

        Notice that we were able to use Tonelli's Theorem to change the order of integration because $X \geq 0$.

      \end{proof}

      We have
      \begin{align*}
        \E[ X \wedge y ] &= \int_{0}^{\infty} (1 - P( \{ X \wedge y \leq x \}) dx \\
        &= \int_{0}^{y} (1 - P( \{ X \leq x \} ) dx \\
        &= \int_{0}^{y} P( \{ X \geq x \} dx
      \end{align*}

      Therefore,
      \[ \frac{d}{dy} \E [ X \wedge y ] = P( \{ X \geq y \} ) .\]

      By our previous claim,
      \begin{align*}
        \E[ (X \wedge y)^2 ] &= 2 \int_{0}^{\infty} x ( 1 - P( \{ X \wedge y \leq x\} ))dx \\
        &= 2 \int_{0}^{y} x ( 1 - P( \{ X \leq x \}) ) dx \\
        &= 2 \int_{0}^{y} x P( \{ X \geq x \} ) dx
      \end{align*}

      Therefore,
      \[ \frac{d}{dy} \E[ (X \wedge y)^2 ] = 2 y P( \{ X \geq y \} ) .\]

      Taking the derivative of the variance gives
      \begin{align*}
        \frac{d}{dy} \sigma^2 ( X \wedge y ) &= \frac{d}{dy} \left( \E[ (X \wedge y)^2 ] - \E[ X \wedge y ]^2 \right) \\
        &= \frac{d}{dy} \E[ (X \wedge y)^2 ] - 2 \E[ X \wedge y ] \frac{d}{dy} \E[ X \wedge y ] \\
        &= 2 P( \{ X \geq y \} ) ( y - \E[ X \wedge y ] ) \\
        &\geq 0
      \end{align*}
      using the fact that $X \wedge y \leq y$, so $\E[ X \wedge y ] \leq y$. Thus $\sigma^2(X \wedge y)$ is an increasing function of $y$.

    \end{solution}

  \item

    \begin{problem}

      Let $X_1, X_2, \dots$ be real valued random variables on the same probability space $(\Omega, \mathcal{F}, P)$, with density functions
      \[ f_{X_n}(x) =
        \begin{cases}
          e^n &\text{on } \left( 0, e^{-n} \left( 1 - \frac{1}{n} \right) \right) \\
          \frac{1}{x^2} &\text{on } (n, \infty)
        \end{cases}
      \]
      and assume that $X_1, X_2, \dots$ are independent. What can you say about $S_n = X_1 + \dots + X_n$ as $n \to \infty$?

    \end{problem}

    \begin{solution}

      Let $A_n = \{ \omega \in \Omega : X_n(\omega) \geq n \}$. Then
      \begin{align*}
        P(A_n) &= \int_{\Omega}^{} \mathbbm{1}_{A_n} dP \\
        &= \int_{n}^{\infty} \frac{1}{x^2} dx \\
        &= \frac{1}{n}
      \end{align*}

      Then
      \[ \sum_{i=1}^\infty \frac{1}{i} = \infty .\]
      Because the $X_n$ are independent, we can apply Borel-Cantelli to get
      \[ P(A) = 1 \]
      where $A = \limsup_{n \to \infty} A_n$. That is, with probability 1, $X_n \geq n$ for infinitely many $n$. Therefore, as $n \to \infty$, $S_n$
      is infinite almost surely.

      Let $B_n = \{ \omega \in \Omega : X_n(\omega) \leq e^{-n} \left( 1 - \frac{1}{n} \right) \}$. Then
      \begin{align*}
        P(B_n) &= \int_{\Omega}^{} \mathbbm{1}_{B_n} dP \\
        &= \int_{0}^{e^{-n}\left(1-\frac{1}{n} \right)} e^n dx \\
        &= 1 - \frac{1}{n}
      \end{align*}

      \[ \sum_{i=1}^\infty 1 - \frac{1}{i} = \infty, \]
      so by Borel Cantelli again, we have that $X_n \leq e^{-n} \left( 1 - \frac{1}{n} \right)$ for infinitely many $n$ almost surely.

      Our contribution from $X_n \leq e^{-n} \left( 1 - \frac{1}{n} \right)$ will be finite because
      \begin{align*}
        \sum_{n=1}^\infty e^{-n} \left( 1 - \frac{1}{n} \right) &\leq \sum_{n=1}^\infty e^{-n} \\
        < \infty
      \end{align*}

    \end{solution}

  \item

    \begin{problem}

      If $X_1, X_2,\dots$ is a sequence of identically distributed real valued random variables on the same probability space, with finite
      expectations, then is it necessarily the case that
      \[ \lim_{n \to \infty} \frac{1}{n} \mathbb{E} \left[ \max_{1 \leq j \leq n} |X_j| \right] = 0 ? \]

    \end{problem}

    \begin{solution}

%      This statement is true. The random variables $X_i$ are identically distributed, so we can define $\mu' = \E[|X_i|]$ for all $i$. $\mu' < \infty$
%      because the $X_i$ have finite expectation. There is a small subtlety in the definitions for this to be true. For a random variable $Y$, $\E[Y] =
%      \E[Y_+] - \E[Y_-]$, where we have split $Y$ into positive and negative parts. This value is only defined when the difference has a meaningful
%      value, that is when at least one of $\E[Y_+]$ and $\E[Y_-]$ is finite. Therefore, $Y$ having finite expectation means both pieces must be
%      finite, and $\E[|Y|] = \E[Y_+] + \E[Y_-] < \infty$. This prevents counterexamples in which integrals are interpreted as principal value
%      integrals.
%
%      For any $n$, we have
%      \begin{align*}
%        %\frac{1}{n} \E[ \max_{1 \leq i \leq n |X_i| ]
%      \end{align*}<++>

      Let $\beta > 0$. Then we have
      \begin{align*}
        \frac{1}{n} \E \left[ \max_{1 \leq j \leq n} |X_j| \right] &= \frac{1}{n} \E \left[ \max_{1 \leq j \leq n} |X_j| \mathbbm{1}_{\{|X_j| \leq \beta\}} + \max_{1 \leq
        j \leq n} |X_j| \mathbbm{1}_{\{|X_j| > \beta\}} \right] \\
        &\leq \frac{\beta}{n} + \frac{1}{n} \E \left[ \max_{1 \leq j \leq n} |X_j| \mathbbm{1}_{\{|X_j|>\beta\}} \right] \\
        &= \frac{\beta}{n} + \frac{1}{n} \E \left[ \sum_{j=1}^n |X_j| \mathbbm{1}_{\{|X_j|>\beta\}} \right] \\
        &= \frac{\beta}{n} + \frac{1}{n} \E \left[ \sum_{j = 1}^n |X_1| \mathbbm{1}_{\{|X_1|>\beta\}} \right] \quad \parbox{5cm}{because the $X_j$ are identically
        distributed} \\
        &= \frac{\beta}{n} + \E \left[ |X_1| \mathbbm{1}_{\{|X_1|>\beta\}} \right]
      \end{align*}

      Taking the limit as $n \to \infty$, we get that for any $\beta > 0$,
      \[ \lim_{n \to \infty} \frac{1}{n} \E \left[ \max_{1 \leq j \leq n} | X_j | \right] \leq \E \left[ |X_1| \mathbbm{1}_{\{|X_1| > \beta\}} \right]
    .\]
      Because $X_1$ has finite expectation, $\E \left[ |X_1| \mathbbm{1}_{\{|X_1|> \beta\}} \right] \to 0$ as $\beta \to \infty$. Therefore,
      \[ \lim_{n \to \infty} \frac{1}{n} \E \left[ \max_{1 \leq j \leq n} |X_j| \right] = 0 .\]

    \end{solution}
\end{enumerate}
\end{document}


