%        File: hw3.tex
%     Created: Wed Nov 02 01:00 PM 2016 C
% Last Change: Wed Nov 02 01:00 PM 2016 C
%

\documentclass[a4paper]{article}

\title{Math 8651 Homework 3 }
\date{11/7/16}
\author{Trevor Steil}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{esint}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{bbm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem*{claim}{Claim}
\newtheorem*{problem}{Problem}
%\newtheorem*{lemma}{Lemma}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\supp}[1]{\mathop{\mathrm{supp}}\left(#1\right)}
\newcommand{\lip}[1]{\mathop{\mathrm{Lip}}\left(#1\right)}
\newcommand{\curl}{\mathrm{curl}}
\newcommand{\la}{\left \langle}
\newcommand{\ra}{\right \rangle}
\renewcommand{\vec}[1]{\mathbf{#1}}

\newenvironment{solution}[1][]{\emph{Solution #1}}

\algnewcommand{\Or}{\textbf{ or }}
\algnewcommand{\And}{\textbf{ or }}

\begin{document}
\maketitle

\begin{enumerate}
  \item
    \begin{problem}

      Let $X_k$ be i.i.d. binomial random variables of the form $\\ {P(X_k = j) = \binom{10}{j} \left( \frac{1}{3} \right)^j \left( \frac{2}{3}
      \right)^{10-j} }$. What is $\lim_{n \to \infty} \frac{T_n}{n}$, where $T_n = \sum_{k=1}^n (X_k)^2$? (Do not use any facts except those from class,
      although you may write a binomial random variable as a sum of other random variables.)

    \end{problem}

    \begin{solution}

      Define the Bernoulli random variables $Y_i$ such that $P(Y_i = 1) = \frac{1}{3}$ and $P(Y_i = 0) = \frac{2}{3}$ for all $i$. $X_k$ is a binomial
      random variable, so we can decompose $X_k$ into a sum of i.i.d. Bernoulli random variables as
      \[ X_k = \sum_{j=1}^{10} Y_j .\]

      From this, we see
      \begin{align*}
        X_k^2 &= \sum_{i=1}^{10} \sum_{j=1}^{10} Y_i Y_j \\
        &= \sum_{i=1}^{10} \sum_{j \neq i} Y_i Y_j + \sum_{i=1}^{10} Y_i^2 \\
        &= \sum_{i=1}^{10} \sum_{j \neq i} Y_i Y_j + \sum_{i=1}^{10} Y_i \quad \parbox{5cm}{because $Y_i$ only takes the values 0 and 1}
      \end{align*}

      Using the linearity of the expectation and the independence of the $Y_i$, we have
      \begin{align*}
        \E[ X_k^2 ] &= \sum_{i=1}^{10} \sum_{j \neq i} \E[ Y_i ] \E[ Y_j ] + \sum_{i=1}^{10} \E[ Y_i ] \\
        &= 90 \left( \frac{1}{3} \right)^2 + \frac{10}{3} \\
        &= \frac{40}{3}
      \end{align*}

      Because the $X_k$ are i.i.d., the random variables $X_k^2$ are as well. By the Strong Law of Large Numbers, we have
      \[ \lim_{n \to \infty} \frac{T_n}{n} = \E[ X_k^2 ] = \frac{40}{3} \ a.s. .\]

    \end{solution}

  \item
    \begin{problem}

      Let $\omega = . \omega_1 \omega_2 \omega_3 \dots$ be the decimal representation of $\omega \in [0,1]$. The triple 4,4,5 is said to occur in
      position $k$ of $\omega$ if $\omega_k = \omega_{k+1} = 4$ and $\omega_{k+2} = 5$. Let $Y_n$ be the total number of times this triple occurs up
      to position $n$. What can you say about the limiting behavior of $Y_n(\omega)$ for a typical $\omega$? (This problem is posed somewhat vaguely
      -- you should fill in the details after deciding what to show.)

    \end{problem}

    \begin{solution}

      Let
      \[ X_k(\omega) =
        \begin{cases}
          1 &\text{if the triple 4,4,5 occurs at position $k$ in $\omega$} \\
          0 &\text{else}
        \end{cases}
      \]

      We have $Y_n = \sum_{k=1}^n X_k$. We would like to say $\frac{Y_n}{n} \to \frac{1}{1000}$ almost surely by using the Strong Law of Large
      Numbers. Unfortunately, we do not have independence of the $X_i$. We get around this by noticing that $X_i$ and $X_j$ are independent if $|i-j|
      \geq 3$ because the three-digit sequences we are looking at starting at $\omega_i$ and $\omega_j$ do not overlap. Using this observation, we
      will split $Y$ into pieces that we can use the Strong Law of Large Numbers on and combine the pieces to get the desired result.

      Let
      \[ Y^{(1)}_n = \sum_{k=0}^{\lceil{\frac{n}{3}}\rceil - 1} X_{3k+1}, \]
      \[ Y^{(2)}_n = \sum_{k=0}^{\lceil{\frac{n}{3}}\rceil - 1} X_{3k+2}, \]
      and
      \[ Y^{(3)}_n = \sum_{k=0}^{\lceil{\frac{n}{3}}\rceil - 1} X_{3k+3}. \]

      By the observation above, $X_1, X_4, X_7, \dots$ are i.i.d. random variables with $E[X_{3k+1}] = \frac{1}{1000}$. By the Strong Law of Large
      Numbers, we have
      \[ \frac{Y^{(1)}_n}{\lceil\frac{n}{3}\rceil} \to \frac{1}{1000} \ a.s. .\]

      Similarly, we have
      \[ \frac{Y^{(2)}_n}{\lceil\frac{n}{3}\rceil} \to \frac{1}{1000} \ a.s. ,\]
      and
      \[ \frac{Y^{(3)}_n}{\lceil\frac{n}{3}\rceil} \to \frac{1}{1000} \ a.s. .\]

      Combining these three, we have
      \begin{align*}
        \lim_{n \to \infty} \frac{Y_n}{n} &= \lim_{n \to \infty} \frac{Y^{(1)}_n + Y^{(2)}_n + Y^{(3)}_n}{n} \\
        &= \frac{1}{1000} \ a.s.
      \end{align*}

      Technically, the above only holds when $n$ is a multiple of 3, but when $n$ is not a multiple of 3, $Y_n$ and $Y^{(1)}_n + Y^{(2)}_n +
      Y^{(3)}_n$ differ by at most 2, and $\lceil \frac{n}{3} \rceil$ and $\frac{n}{3}$ are approximately equal. So the limit above will hold.

    \end{solution}

  \item
    \begin{problem}
      Let $X_1, X_2, \dots$ be i.i.d. random variables with $P(X_1 = 2^k) = 2^{-k}, k \geq 1$. Show that for appropriate $c_n$, $\frac{S_n - c_n}{n
      \log n} \to 0$ in probability, where $S_n = \sum_{k=1}^n X_k$.
    \end{problem}

    \begin{proof}

      We will be using Theorem 2.2.6 from Durrett with $b_n = n \log n$.

      Let
      \[ \bar{X_n} = X_n \mathbbm{1}_{\{X_n \leq n \log n\} } .\]
      First, we compute
      \begin{align*}
        P( |X_k| \geq 2^n ) &= \sum_{m=n}^\infty 2^{-m} \\
        &= 2^{-n+1}
      \end{align*}
      Using this, we are able to see
      \begin{align*}
        n P ( |X_n| \geq n \log n ) &= n 2^{- \log( n \log n) + 1} \\
        &= 2n 2^{-\log ( n \log n ) } \\
        &= \frac{2n}{n \log n} \\
        &= \frac{2}{\log n}
      \end{align*}
      Therefore,
      \[ \lim_{n \to \infty} P( |X_k| \geq n \log n ) = 0,\]
      so (i) from the theorem is satisfied.

      \begin{align*}
        \E [\bar{X_n}^2] &= \sum_{k=1}^{\log ( n \log n )} 2^{-k} 2^{2k} \\
        &= \sum_{k=1}^{\log ( n \log n )} 2^k \\
        &= 2^{\log( n \log n ) + 1} \\
        &= 2 n \log n
      \end{align*}

      Therefore,
      \[ \frac{1}{n^2 \log^2 n} n \E[ \bar{X_n}^2 ] = \frac{2}{\log n} \to 0, \]
      and (ii) from the theorem is satisfied.

      So by Theorem 2.2.6,
      \[ \frac{S_n - c_n}{n \log n} \to 0 \text{ in probability} \]
      for
      \begin{align*}
        c_n &= n \E[\bar{X}_1] \\
        &= n \sum_{m=1}^{n \log n} 2^{-m} 2^{m} \\
        &= n^2 \log n
      \end{align*}

    \end{proof}

  \item
    \begin{problem}
      Let $S_n$ be chosen as in the previous problem. Show that there is no choice of $c_n$ so that $\frac{S_n - c_n}{n} \to 0$ in probability.
    \end{problem}

    \begin{proof}

      By a remark in Durrett following the statement of the Weak Law of Large Numbers (Theorem 2.2.7), in order to have
      \[ \frac{S_n - a_n}{n} \to 0 \text{ in probability } \]
      for some constants $a_n$, it is necessary that
      \[ x P(|X_1| > x) \to 0 \text{ as } x \to \infty .\]

      In our case, we have
      \[ 2^{k} P( X_1 > 2^k ) = 1 \not \to 0 .\]
      Therefore, we cannot have constants $a_n$ such that $\frac{S_n - a_n}{n} \to 0$ in probability.

    \end{proof}

\end{enumerate}
\end{document}


