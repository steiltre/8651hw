%        File: hw4.tex
%     Created: Thu Dec 01 01:00 PM 2016 C
% Last Change: Thu Dec 01 01:00 PM 2016 C
%

\documentclass[a4paper]{article}

\title{Math 8651 Homework 4 }
\date{12/7/16}
\author{Trevor Steil}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{esint}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{bbm}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem*{claim}{Claim}
\newtheorem*{problem}{Problem}
%\newtheorem*{lemma}{Lemma}
\newtheorem{definition}{Definition}[section]

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\supp}[1]{\mathop{\mathrm{supp}}\left(#1\right)}
\newcommand{\lip}[1]{\mathop{\mathrm{Lip}}\left(#1\right)}
\newcommand{\curl}{\mathrm{curl}}
\newcommand{\la}{\left \langle}
\newcommand{\ra}{\right \rangle}
\renewcommand{\vec}[1]{\mathbf{#1}}

\newenvironment{solution}[1][]{\emph{Solution #1}}

\algnewcommand{\Or}{\textbf{ or }}
\algnewcommand{\And}{\textbf{ or }}

\begin{document}
\maketitle
\begin{enumerate}
  \item
    \begin{problem}
      Let $X_k, Y_k, k=1,2, \dots$ and $X,Y$ be random variables on the same probability space $(\Omega, F, P)$.
      \begin{enumerate}
        \item If $X_k \to X$ in distribution and $Y_k \to Y$ in distribution as $k \to \infty$, is it necessarily true that $(X_k, Y_k) \to (X,Y)$ in
          distribution? (Interpret this to mean $\E [ f(X_k, Y_k) ] \to \E[ f(X,Y) ]$ as $k \to \infty$ for all bounded, continuous $f: \R^2 \to \R$.)

        \item If $X_k \to X$ and $Y_k \to Y$ in probability, is it necessarily true that $(X_k,Y_k) \to (X,Y)$ in probability?

      \end{enumerate}

    \end{problem}

    \begin{solution}

      \begin{enumerate}
        \item
          This claim is not true. Let $X_k$ be iid copies of a random variable $X$ that takes the value 1 with probability $\frac{1}{2}$ and 0 with
          probability $\frac{1}{2}$. Let $Y_k = 1 - X_k$. We see that $X_k$ and $Y_k$ have the same distribution but are not independent.
          Then $X_k \to X$ in distribution because all of the $X_k$ have the same distribution.
          Similarly, $Y_k \to Y$ in distribution, where we will let $X$ and $Y$ be independent.

          Take the continuous function $f(x,y) = e^{x + y}$. Then for any $k$,
          \begin{align*}
            f(X_k, Y_k) &= e^{X_k + Y_k} \\
            &= e
          \end{align*}
          on all of $\Omega$. Therefore,
          \[ \E[f(X_k, Y_k)] = e \]
          for all $k$.

          For $e^{X+Y}$, we have $P(e^{X+Y}=1)=\frac{1}{4}$, $P(e^{X+Y} = e) = \frac{1}{2}$, and $P(e^{X+Y} = e^2) = \frac{1}{4}$ because $X$ and $Y$
          are independent. Therefore,
          \begin{align*}
            \E[e^{X+Y}] &= \frac{1}{4} + \frac{1}{2}e + \frac{1}{4} e^2 \\
            &\neq e \\
            &= \lim_{k \to \infty} \E[e^{X_k + Y_k}]
          \end{align*}

        \item
          Assume $X_n \to X$ and $Y_n \to Y$ in probability. We are interested in $\{ |(X_n, Y_n) - (X,Y)| > \varepsilon \}$. If $|(X_n, Y_n) - (X,Y)| > \varepsilon$,
          then $|X_n - X| > \frac{\sqrt{2}}{2} \varepsilon$ or $|Y_n - Y| > \frac{\sqrt{2}}{2} \varepsilon$. This can be proved by contradiction. If
          $|X_n-X|, |Y_n-Y| \leq \frac{\sqrt{2}}{2} \varepsilon$, then
          \begin{align*}
            |(X_n, Y_n) - (X,Y)| &= \sqrt{|X_n-X|^2 + |Y_n-Y|^2} \\
            &\leq \sqrt{\frac{1}{2} \varepsilon^2 + \frac{1}{2} \varepsilon^2} \\
            &= \varepsilon
          \end{align*}
          Therefore, if $|(X_n, Y_n) - (X, Y)| > \varepsilon$, we must have $|X_n - X| > \frac{\sqrt{2}}{2} \varepsilon$ or $|Y_n - Y| > \frac{\sqrt{2}}{2}
          \varepsilon$.

          This means
          \[ \{ |(X_n, Y_n) - (X,Y)| > \varepsilon \} \subseteq \left( \{ |X_n - X| > \frac{\sqrt{2}}{2} \varepsilon \} \times \Omega \right) \cup
            \left( \{ |Y_n - Y| > \frac{\sqrt{2}}{2} \varepsilon \} \times \Omega \right) .\]

          If we take probabilities, we have
          \[ P \left( \{ |(X_n, Y_n) - (X,Y)| > \varepsilon \} \right) \leq P \left( \{ |X_n - X| > \frac{\sqrt{2}}{2} \varepsilon \} \right) +
          P \left( \{ |Y_n - Y| > \frac{\sqrt{2}}{2} \varepsilon \} \right) \]
          because $P(\Omega) = 1$ (we have abused notation slightly here and are using $P$ for the measure on our original probability space as well
          as the product space). Both terms on the right go to 0 as $n \to \infty$ because $X_n \to X$ and $Y_n \to Y$ in probability, so
          \[ \lim_{n \to \infty} P \left( \{ |(X_n, Y_n) - (X,Y)| > \varepsilon \} \right) = 0, \]
          and $(X_n, Y_n) \to (X,Y)$ in probability.

      \end{enumerate}

    \end{solution}

  \item
    \begin{problem}
      Show that the characteristic function of the Poisson distribution with parameter $\lambda$ is $\exp \left( \lambda( e^{it} - 1) \right)$.
    \end{problem}

    \begin{proof}

      Let $X$ be a random variable with a Poisson distribution. We know the probability mass function of $X$ is given by $\frac{\lambda^k
        e^{-\lambda}}{k!}$. Therefore, we can compute
        \begin{align*}
          \varphi(t) &= \E [ e^{itX} ] \\
          &= \sum_{k=0}^\infty e^{itk} \frac{\lambda^k e^{-\lambda}}{k!} \\
          &= e^{-\lambda} \sum_{k=0}^\infty \frac{\left( \lambda e^{it} \right)^k}{k!} \\
          &= e^{-\lambda} \exp \left( \lambda e^{it} \right) \quad \parbox{5cm}{by the power series for $e^x$} \\
          &= \exp \left( \lambda \left( e^{it} - 1 \right) \right)
        \end{align*}

    \end{proof}

  \item
    \begin{problem}
      Suppose that $X$ and $Y$ are independent random variables and that $X$ and $X+Y$ have the same distribution. What can you say about $Y$?
      Explain.
    \end{problem}

    \begin{solution}
      Because $X$ and $X+Y$ have the same distribution. They have the same characteristic function as well. Therefore,
      \begin{align*}
        \varphi_X(t) &= \varphi_{X+Y} (t) \\
        &= \varphi_X(t) \varphi_Y(t) \quad \parbox{5cm}{by independence}
      \end{align*}
      for all $t$. Therefore, we have
      \[ 0 = \varphi_X(t) \left( 1 - \varphi_Y(t) \right) .\]

      $\varphi_X$ is a characteristic function, so $\varphi(0) = 1$ and $\varphi$ is continuous. In particular, there is a neighborhood $U$ of 0 such
      that $\varphi(t) \neq 0$ for all $t \in U$. Therefore, $\varphi_Y(t) = 1$ for all $t \in U$.

      By definition, $\varphi_Y(t) = \int_{\Omega}^{} e^{itY} dP$. The only way for this
      to equal $1$ for all $t \in U$ is for $Y=0$ almost surely. Thus, for $X$ and $X+Y$ to have the same distribution, $Y=0$ almost surely.
    \end{solution}

  \item
    \begin{problem}
      Suppose that $\varphi(t)$ is a characteristic function. Is it necessarily true that $|\varphi(t)|$ is a characteristic function? (Hint: consider
      $\cos t$.
    \end{problem}

    \begin{solution}

      Consider the discrete random variable with $P(X=-1) = P(X=1) = \frac{1}{2}$. Then
      \begin{align*}
        \E [ e^{itX} ] &= \int_{\R}^{} e^{itX} dP \\
        &= \frac{1}{2} \left( e^{-it} + e^{it} \right) \\
        &= \cos t
      \end{align*}

      Therefore, $\cos t$ is the characteristic function for this random variable.

      Now we must show $| \cos t |$ is not a characteristic function. Assume it is. Let $\mu$ be the associated measure. By Theorem 6.2.4 in Chung, we
      have
      \[ \mu(x) = \lim_{T \to \infty} \frac{1}{2T} \int_{-T}^{T} e^{-itx} | \cos t | dt .\]
      Because $\cos t$ and $e^{-itx}$ are both $2 \pi$ periodic, we can simplify this to
      \[ \mu(x) = \frac{1}{2 \pi} \int_{0}^{2\pi} e^{-itx} | \cos t | dt .\]

      We recognize this as the expression for the $x^{th}$ Fourier coefficients for $| \cos t |$. We can write the Fourier series as a cosine series because $| \cos t |$ is an even
      function. The series that we get is
      \[ | \cos t | = \frac{2}{\pi} + \frac{4}{\pi} \sum_{m=1}^\infty \frac{(-1)^m}{1 - 4m^2} \cos(2mt) . \]

      We now need to find a negative coefficient in this series. By looking at the second term (and noticing our series had odd terms drop out), we
      get
      \begin{align*}
        \mu(4) &= \frac{4}{\pi} \frac{(-1)^2}{1 - 4 * 2^2} \\
        &= - \frac{4}{15 \pi}
      \end{align*}
      which contradicts $\mu$ being a measure.

    \end{solution}

  \item
    \begin{problem}
      The Central Limit Theorem states that under appropriate conditions on IID random variables $X_1, X_2, \dots, \frac{S_n}{\sqrt{n}} \to Z$ weakly,
      where $Z$ is $N(0,1)$. Why or why not can this convergence be in probability or almost surely?
    \end{problem}

    \begin{solution}

      The convergence in the Central Limit Theorem cannot be in probability. Assume $\frac{1}{\sqrt{n}}S_n \to Z$ in probability. Then we would have
      $\frac{1}{\sqrt{2n}}S_{2n} \to Z$ in probability as well.

      Define
      \[ W_n = \frac{S_{2n} - S_n}{\sqrt{2n}} \]
      and
      \[ Y_n = \frac{S_n}{\sqrt{n}} \left( \frac{1}{\sqrt{2}} - 1 \right) .\]
      By definition, $W_n$ and $Y_n$ are independent because $W_n$ contains a sum of $X_k$ for $k \leq n$, and $Y_n$ contains a sum of $X_k$ for
      $k>n$.

      We have that
      \begin{align*}
        W_n - Y_n &= \frac{S_{2n} - S_n}{\sqrt{2n}} - \frac{S_n}{\sqrt{n}} \left( \frac{1}{\sqrt{2}} - 1 \right) \\
        &= \frac{S_{2n}}{\sqrt{2n}} - \frac{S_n}{\sqrt{n}}
      \end{align*}

      This means
      \begin{align*}
        |W_n - Y_n| &= \left| \frac{S_{2n}}{\sqrt{2n}} - \frac{S_n}{\sqrt{n}} \right| \\
        &\leq \left| \frac{S_{2n}}{\sqrt{2n}} - Z \right| + \left| \frac{S_n}{\sqrt{n}} - Z \right|
      \end{align*}

      Therefore,
      \begin{align*}
        P \left( \left| W_n - Y_n \right| > 2 \varepsilon \right) &\leq P \left( \left| \frac{S_{2n}}{\sqrt{2n}} - Z \right| > \varepsilon \right) + P
        \left( \left| \frac{S_n}{\sqrt{n}} - Z \right| > \varepsilon \right) \\
        &\quad \to 0 \text{ as } n \to \infty
      \end{align*}
      by our assumption of convergence in probability.

      Therefore, the $W_n$ and $Y_n$ are independent and growing closer in probability. This means $W_n$ and $Y_n$ are growing closer to constant. But
      we know $W_n, Y_n \to Z$ in distribution, which is a contradiction. Thus, we cannot get convergence in probability from the Central Limit
      Theorem.

      We also can't get almost sure convergence because almost sure convergence implies convergence in probability. So if we could, we would be given
      convergence in probability as well, which we just established isn't possible.

    \end{solution}

\end{enumerate}
\end{document}


